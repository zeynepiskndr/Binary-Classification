{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1svz7dgamH55VxyUICniY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeynepiskndr/zeynep/blob/master/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lt1_noiww08"
      },
      "source": [
        "#In[]\n",
        "#1-Import\n",
        "import tensorflow.compat.v1 as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import gensim\n",
        "import datetime\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.externals import joblib\n",
        "%matplotlib inline\n",
        "\n",
        "#In[]\n",
        "#2-Check GPU is running\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "#In[]\n",
        "#3-Activating eager execution\n",
        "tf.enable_eager_execution()\n",
        "tf.executing_eagerly()\n",
        "\n",
        "#In[]\n",
        "#4-Defining path\n",
        "path=\"/Users/emirh/Desktop/bitirme\"\n",
        "\n",
        "#In[]\n",
        "#5-Reading dataset file\n",
        "df = pd.DataFrame()\n",
        "df = pd.read_csv(path+'/movie_data.csv', encoding='utf-8')\n",
        "df.head(3)\n",
        "\n",
        "#In[]\n",
        "#6-Removing punctuations and stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "example_sent = \"This is not a sample sentence, showing off the stop words filtration.\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "#Remove critical stopwords\n",
        "stop_words.remove('no')\n",
        "stop_words.remove('not')\n",
        "stop_words.remove(\"wasn't\")\n",
        "stop_words.remove(\"isn't\")\n",
        "stop_words.remove('against')\n",
        "\n",
        "word_tokens = word_tokenize(example_sent) \n",
        "  \n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  \n",
        "filtered_sentence = []\n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  \n",
        "print(word_tokens) \n",
        "print(filtered_sentence) \n",
        "\n",
        "#In[]\n",
        "#7-Tokenize\n",
        "review_lines = list()\n",
        "lines = df['review'].values.tolist()\n",
        "\n",
        "for line in lines:   \n",
        "    tokens = word_tokenize(line)\n",
        "    # convert to lower case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # remove punctuation from each word    \n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    # filter out stop words    \n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    review_lines.append(words)\n",
        "\n",
        "#In[]\n",
        "#8-Train word2vec model\n",
        "EMBEDDING_DIM = 250\n",
        "# train word2vec model\n",
        "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=8, workers=5, min_count=15,negative=5,iter=20)\n",
        "# vocab size\n",
        "words = list(model.wv.vocab)\n",
        "print('Vocabulary size: %d' % len(words))\n",
        "\n",
        "#In[]\n",
        "#9-Save model in ASCII (word2vec)format\n",
        "filename = path+'/imdb_embedding_word2vec.txt'\n",
        "model.wv.save_word2vec_format(filename, binary=False)\n",
        "\n",
        "#In[]\n",
        "#10-Word embedding\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('', path+'/imdb_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "max_length = 250 # assuming documents are 500 words\n",
        "\n",
        "#In[]\n",
        "#11-Prepare train and test set\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer_obj = Tokenizer(num_words=50000)\n",
        "#tokenizer_obj.num_words=50000 #50000 most frequent words will be kept\n",
        "tokenizer_obj.fit_on_texts(review_lines)\n",
        "#print (\"tokenizer_obj.word_count \",tokenizer_obj.word_counts)\n",
        "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
        "\n",
        "# pad sequences\n",
        "word_index = tokenizer_obj.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
        "sentiment =  df['sentiment'].values\n",
        "print('Shape of review tensor:', review_pad.shape)\n",
        "print('Shape of sentiment tensor:', sentiment.shape)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(review_pad.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "review_pad = review_pad[indices]\n",
        "sentiment = sentiment[indices]\n",
        "\n",
        "#VALIDATION_SPLIT = 0.2\n",
        "#num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
        "#X_train_pad = review_pad[:-num_validation_samples]\n",
        "#y_train = sentiment[:-num_validation_samples]\n",
        "#X_test_pad = review_pad[-num_validation_samples:]\n",
        "#y_test = sentiment[-num_validation_samples:]\n",
        "\n",
        "X_train_pad = review_pad[:40000]\n",
        "y_train = sentiment[:40000]\n",
        "X_eval_pad = review_pad[40001:45000]\n",
        "y_eval = sentiment[40001:45000]\n",
        "X_test_pad = review_pad[45001:]\n",
        "y_test = sentiment[45001:]\n",
        "\n",
        "#In[]\n",
        "#12\n",
        "print('X_train_pad tensor:', X_train_pad[1])\n",
        "print('y_train tensor:', y_train[1])\n",
        "\n",
        "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
        "print('Shape of y_test tensor:', y_test.shape)\n",
        "\n",
        "#In[]\n",
        "#13-This is a lookup dictionary for embedding\n",
        "EMBEDDING_DIM = 250\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#In[]\n",
        "#14\n",
        "print('Words = '+str(num_words))\n",
        "print('Embedding matrix = '+str(len(+embedding_matrix)))\n",
        "\n",
        "#In[]\n",
        "#15 Creating model to train\n",
        "from tensorflow.compat.v1 import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "filepath=path+\"/best_model.pkl\"\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=0.01, mode='min', verbose=1, patience=200)\n",
        "mc = ModelCheckpoint(filepath, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "#'weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
        "\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)\n",
        "def createModelStructure():\n",
        "    model = Sequential([\n",
        "    embedding_layer,\n",
        "    LSTM(units=64, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(500, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "    Dropout(0.2),\n",
        "    Dense(300, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "model = createModelStructure()\n",
        "\n",
        "print('Summary of the built model...')\n",
        "print(model.summary())\n",
        "\n",
        "#In[]\n",
        "#16 - Train\n",
        "print('Train...')\n",
        "\n",
        "model.fit(X_train_pad, y_train, batch_size=512, epochs=60, validation_data=(X_eval_pad, y_eval), callbacks=[tensorboard_callback,es,mc])\n",
        "#In[]\n",
        "#17-Load model to new model\n",
        "saved_model = createModelStructure()\n",
        "saved_model.load_weights(filepath)\n",
        "#In[]\n",
        "filename = 'finalized_model.pkl'\n",
        "joblib.dump(saved_model, filename)\n",
        "saved_model = joblib.load(path+'/'+filename)\n",
        "#In[]\n",
        "#18-Load Tensorboard extension\n",
        "%load_ext tensorboard\n",
        "#In[]\n",
        "#19\n",
        "from tensorboard import notebook\n",
        "notebook.list()\n",
        "#In[]\n",
        "#20\n",
        "notebook.display(port=6006, height=1000) \n",
        "#In[]\n",
        "#21\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "#In[]\n",
        "#22\n",
        "print('Testing...')\n",
        "score, acc = saved_model.evaluate(X_test_pad, y_test, batch_size=512)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "print(\"Accuracy: {0:.2%}\".format(acc))\n",
        "#In[]\n",
        "#23\n",
        "y_pred = saved_model.predict(X_test_pad[:4999], batch_size=512)\n",
        "y_pred=(y_pred>0.6)\n",
        "#In[]\n",
        "#24\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "average_precision = average_precision_score(y_test[:4999], y_pred)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "#In[]\n",
        "#25\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test[:4999], y_pred.round())\n",
        "\n",
        "print(cm)\n",
        "#In[]\n",
        "#26\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test[:4999], y_pred.round()))\n",
        "\n",
        "#In[]\n",
        "#27\n",
        "#Let us test some  samples\n",
        "test_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
        "test_sample_2 = \"Good movie!\"\n",
        "test_sample_3 = \"Maybe I like this movie.\"\n",
        "test_sample_4 = \"Not to my taste, will skip and watch another movie\"\n",
        "test_sample_5 = \"if you like action, then this movie might be good for you.\"\n",
        "test_sample_6 = \"Bad movie!\"\n",
        "test_sample_7 = \"Not a good movie!\"\n",
        "test_sample_8 = \"This movie really sucks! Can I get my money back please?\"\n",
        "test_sample_9 = \"Not bad\"\n",
        "test_sample_10 = \"Great actors\"\n",
        "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8, test_sample_9, test_sample_10]\n",
        "\n",
        "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
        "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n",
        "\n",
        "#predict\n",
        "y_pred = saved_model.predict(x=test_samples_tokens_pad)\n",
        "for i in y_pred:\n",
        "    if i>0.54:\n",
        "        print('Positive')\n",
        "    else:\n",
        "        print('Negative')\n",
        "#In[]\n",
        "filename = 'finalized_model.pkl'\n",
        "joblib.dump(saved_model, filename)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}